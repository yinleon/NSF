{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = [str(x) for x in range(2010,2017)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the NSF\n",
    "Use BeautifulSoup to crawl the NSF site and extract awards, award recipients (PI's), univerities and NSF internal divisions. Dump'em into JSON for analysis/warehousing...\n",
    "<br>Currently only one year at a time-- how to scale up + do it all in memory?(!?!)\n",
    "<br>TODO: use multiprocessing pool to speed up crawls!\n",
    "<br><t> cut doops by checking if key exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 13092 records in 114.8 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# iterate through each year\\nfor path in paths:\\n    main(path)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate globals\n",
    "def main(path):\n",
    "    outfiles = [\"NSF_AWARDS_\"+path+\".json\",\n",
    "                \"NSF_ABSTRACT_\"+path+\".json\",\n",
    "                \"NSF_INSTITUTIONS_\"+path+\".json\",\n",
    "                \"NSF_PI_\"+path+\".json\",\n",
    "                \"NSF_DIVISION_\"+path+\".json\"]\n",
    "   \n",
    "    d1_list = []\n",
    "    d2_list = []\n",
    "    d3_list = []\n",
    "    d4_list = []\n",
    "    d5_list = []\n",
    "\n",
    "    def xml_parse(file):\n",
    "        handler = open(file,'r',encoding='utf8').read()\n",
    "        # get the xml file into soup\n",
    "        soup = BeautifulSoup(handler,\"lxml\")\n",
    "        soup.prettify()\n",
    "\n",
    "        # sectional soups\n",
    "        soup2= soup.find(\"institution\")\n",
    "        soup3 = soup.find(\"investigator\")\n",
    "        soup4 = soup.find(\"organization\")\n",
    "        \n",
    "        \n",
    "        # awards\n",
    "        try:\n",
    "            award = soup.find(\"awardid\").string\n",
    "            university = soup2.find(\"name\").string\n",
    "            nsf_division = soup4.find(\"division\").find(\"longname\").string\n",
    "            email = soup3.find(\"emailaddress\").string\n",
    "            d1_list.append(\n",
    "                OrderedDict(\n",
    "                    [('award_id',award),\n",
    "                     ('award_title',soup.find(\"awardtitle\").string),\n",
    "                     ('award_effective_date',soup.find(\"awardeffectivedate\").string),\n",
    "                     ('award_expiration_date',soup.find(\"awardexpirationdate\").string),\n",
    "                     ('award_amount',soup.find(\"awardamount\").string),\n",
    "                     ('institution_name',university),\n",
    "                     ('division',nsf_division),\n",
    "                     ('email_address',email)]\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            return\n",
    "        # abstract\n",
    "        try:\n",
    "            d2_list.append(\n",
    "                OrderedDict([\n",
    "                    ('award_id',award),\n",
    "                    ('abstract_narration',soup.find('abstractnarration').string.strip())]\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            return\n",
    "\n",
    "        # institution\n",
    "        try:\n",
    "            d3_list.append(\n",
    "                OrderedDict(\n",
    "                    [('institution_name',university),\n",
    "                     ('city_name',soup2.find(\"cityname\").string),\n",
    "                     ('phone_number',soup2.find(\"phonenumber\").string),\n",
    "                     ('street_address',soup2.find(\"streetaddress\").string),\n",
    "                     ('country_name',soup2.find(\"countryname\").string),\n",
    "                     ('state_name',soup2.find(\"statename\").string),\n",
    "                     ('state_code',soup2.find(\"statecode\").string)]\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            return\n",
    "\n",
    "        # PI\n",
    "        try:\n",
    "            d4_list.append(\n",
    "                OrderedDict(\n",
    "                    [('first_name',soup3.find(\"firstname\").string),\n",
    "                     ('last_name',soup3.find(\"lastname\").string),\n",
    "                     ('email_address',email),\n",
    "                     ('institution_name',soup2.find(\"name\").string)]\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            return\n",
    "\n",
    "        # Division\n",
    "        try:\n",
    "            d5_list.append(\n",
    "                OrderedDict(\n",
    "                    [('code',soup4.find(\"code\").string),\n",
    "                     ('directorate',soup4.find(\"directorate\").find(\"longname\").string),\n",
    "                     ('division',nsf_division)]\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    def json_dump():\n",
    "        dict_NSF = [d1_list, d2_list, d3_list, d4_list, d5_list]\n",
    "        for i in range(len(dict_NSF)):\n",
    "            # remove duplicates\n",
    "            dict_NSF[i] = [j for n, j in enumerate(dict_NSF[i]) if j not in dict_NSF[i][n + 1:]]\n",
    "            with open(outfiles[i], 'w') as outfile:\n",
    "                json.dump(dict_NSF[i], outfile, indent=4)\n",
    "\n",
    "    # main script\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "    # iterate through the path directory\n",
    "    for file in os.listdir('data/'+path):\n",
    "        xml_parse(os.path.join('data/'+path,file))\n",
    "        count += 1\n",
    "    print(\"Scraped\", count,\"records in\", round(time.time()-start, 2), \"sec.\")   \n",
    "    json_dump()\n",
    "    #TODO: create a SQL or noSQL dump!\n",
    "\n",
    "# call for each year\n",
    "main('2010')\n",
    "\n",
    "'''\n",
    "# iterate through each year\n",
    "for path in paths:\n",
    "    main(path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of NSF 2015 \n",
    "Cool things to look at\n",
    "<br> Common words in titles\n",
    "<br> email each PI with a status update.\n",
    "<br> Chloropleth about instituional funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "award_df = pd.read_json(outfiles[0])\n",
    "abstract_df = pd.read_json(outfiles[1])\n",
    "org_df = pd.read_json(outfiles[2])\n",
    "pi_df = pd.read_json(outfiles[3])\n",
    "div_df = pd.read_json(outfiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "award_df.merge(div_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RDBA this works great... However, for publishing results nobody wants columns displayed_like_this! So here's a helper function to convert underscored columns into respectible, well-mannered headers. Appropriated from Stackoverflow user <a target=\"_blank\" href='http://stackoverflow.com/a/6425628/5094480'>Siegfried Gevatter</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beautify(underscored_word):\n",
    "    # check for acronym\n",
    "    if(len(underscored_word)>2):\n",
    "        return ' '.join(x.capitalize() or '_' for x in underscored_word.split('_'))\n",
    "    else:\n",
    "        return underscored_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, flexible reporting function to find the top n funded column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def top_funds(col,n):\n",
    "    top_fund_col = []\n",
    "    for uni in award_df[col].unique():\n",
    "        top_fund_col.append(\n",
    "            {beautify(col):uni, \"Total Award Money\" : award_df[award_df[col]==uni].award_amount.sum()})\n",
    "    fund_df = pd.DataFrame(sorted(top_fund_col, key=lambda k: k['Total Award Money'], reverse=True))\n",
    "    fund_df.index = fund_df.index+1\n",
    "    return fund_df[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_funds('institution_name',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_funds('division',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'award_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8b0b9f0aa2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfund_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeautify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfund_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total Award Money\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PI\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Email Address\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Funding Division\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtop_PI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-8b0b9f0aa2d1>\u001b[0m in \u001b[0;36mtop_PI\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO fix PI shared names with email addresses!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop_PI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maward_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PI'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maward_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maward_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpi_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PI'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpi_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtop_fund_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'award_df' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO fix PI shared names with email addresses!\n",
    "def top_PI():\n",
    "    award_df['PI'] = award_df.first_name+\" \"+award_df.last_name\n",
    "    pi_df['PI'] = pi_df.first_name+\" \"+pi_df.last_name\n",
    "    top_fund_col = []\n",
    "    for PI in pi_df.PI:\n",
    "        top_fund_col.append(\n",
    "            {'PI':PI, 'funding_division': np.asarray(award_df[award_df.PI==PI].division)[0],\"Total_Award_Money\" : award_df[award_df['PI']==PI].award_amount.sum()})\n",
    "    fund_df = pd.DataFrame(sorted(top_fund_col, key=lambda k: k['Total_Award_Money'], reverse=True))\n",
    "    fund_df = pd.merge(left=fund_df,right=pi_df,on='PI',how='inner')\n",
    "    fund_df.index = fund_df.index+1\n",
    "    fund_df.rename(columns=lambda x: beautify(x), inplace=True)\n",
    "    return fund_df[[\"Total Award Money\",\"PI\",\"Email Address\",\"Funding Division\"]][:25].drop_duplicates()\n",
    "top_PI()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
