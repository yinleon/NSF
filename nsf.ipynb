{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "from lxml import etree as ET\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the NSF\n",
    "Use BeautifulSoup to crawl the NSF site and extract awards, award recipients (PI's), univerities and NSF internal divisions. Dump'em into JSON/CSV for analysis/warehousing...\n",
    "<br>TODO: use multiprocessing pool to speed up crawls!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from Ivan:\n",
    "Use Lxml rather than b4\n",
    "don't fund before use\n",
    "Classes!\n",
    "Declarative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 2000 ***\n",
      "*** 2001 ***\n",
      "*** 2002 ***\n",
      "*** 2003 ***\n",
      "*** 2004 ***\n",
      "*** 2005 ***\n",
      "*** 2006 ***\n",
      "*** 2007 ***\n",
      "*** 2008 ***\n",
      "*** 2009 ***\n",
      "*** 2010 ***\n",
      "*** 2011 ***\n",
      "*** 2012 ***\n",
      "*** 2013 ***\n",
      "*** 2014 ***\n",
      "*** 2015 ***\n",
      "*** 2016 ***\n",
      "185345 records\n",
      "Scraped in 278.76 sec.\n",
      "****************\n",
      "Duplicates removed in 11.1 sec.\n",
      "Dumped into CSV in 27.18 sec.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # remove stuff I don't want...\n",
    "    rep = {\"Division \": \"\", \"Of \": \"\",\"of \": \"\", \"Div \": \"\",\"Directorate \":\"\",\"Divn \":\"\",\"for \":\"\"}\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    paths = [str(x) for x in range(2000,2017)]\n",
    "\n",
    "    def scrape(path,MySQL=False):\n",
    "        # Builder functions\n",
    "        def awards_builder(tree):\n",
    "            return awards.append(\n",
    "                    OrderedDict([\n",
    "                            ('AwardID',tree.find('Award/AwardID').text),\n",
    "                            ('AwardTitle',tree.find('Award/AwardTitle').text),\n",
    "                            ('AwardEffectiveDate',tree.find('Award/AwardEffectiveDate').text),\n",
    "                            ('AwardExpirationDate',tree.find('Award/AwardExpirationDate').text),\n",
    "                            ('AwardAmount',tree.find('Award/AwardAmount').text),\n",
    "                            ('InstitutionName',tree.find('Award/Institution/Name').text),\n",
    "                            ('Division',pattern.sub(lambda m: rep[re.escape(m.group(0))],\n",
    "                                                    tree.find('Award/Organization/Division')[0].text)),\n",
    "                            ('EmailAddress',tree.find('Award/Investigator/EmailAddress').text)\n",
    "                    ]))\n",
    "\n",
    "        def abstracts_builder(tree):\n",
    "            return abstracts.append(\n",
    "                    OrderedDict([\n",
    "                            ('AwardID',tree.find('Award/AwardID').text),\n",
    "                            ('Abstract',tree.find('Award/AbstractNarration').text)\n",
    "                     ]))\n",
    "\n",
    "        def institutions_builder(tree):\n",
    "            return institutions.append(\n",
    "                    OrderedDict([\n",
    "                            ('InstitutionName',tree.find('Award/Institution/Name').text),\n",
    "                            ('StreetAddress',tree.find('Award/Institution/StreetAddress').text),\n",
    "                            ('CityName',tree.find('Award/Institution/CityName').text),\n",
    "                            ('StateCode',tree.find('Award/Institution/StateCode').text),\n",
    "                            ('ZipCode',tree.find('Award/Institution/ZipCode').text),\n",
    "                            ('CountryName',tree.find('Award/Institution/CountryName').text)\n",
    "                    ]))\n",
    "        # what if two pis, what if pi moves universities?\n",
    "        def PI_builder(tree):\n",
    "            return PI.append(\n",
    "                    OrderedDict([\n",
    "                            ('EmailAddress',tree.find('Award/Investigator/EmailAddress').text),\n",
    "                            ('FirstName',tree.find('Award/Investigator/FirstName').text),\n",
    "                            ('LastName',tree.find('Award/Investigator/LastName').text)\n",
    "                    ]))\n",
    "\n",
    "        def xml_parse(file):\n",
    "            tree = ET.parse(file)\n",
    "            try:\n",
    "                awards_builder(tree)\n",
    "            except:\n",
    "                return(tree.find('Award/AwardID').text,\"is missing a award key\")\n",
    "\n",
    "            abstracts_builder(tree)\n",
    "\n",
    "            institutions_builder(tree)\n",
    "\n",
    "            PI_builder(tree)\n",
    "\n",
    "        # main script\n",
    "        print(\"***\",path,\"***\")\n",
    "        # iterate through the path directory\n",
    "        [xml_parse(os.path.join('data/'+path,file)) for file in os.listdir('data/'+path)]\n",
    "        return\n",
    "\n",
    "    def json_dump():\n",
    "        dict_NSF = [awards,\n",
    "                    abstracts,\n",
    "                    institutions,\n",
    "                    PI]\n",
    "\n",
    "        outfiles = [\"NSF_AWARDS.json\",\n",
    "                    \"NSF_ABSTRACTS.json\",\n",
    "                    \"NSF_INSTITUTIONS.json\",\n",
    "                    \"NSF_PI.json\"]\n",
    "\n",
    "        for i in range(len(dict_NSF)):\n",
    "            with open(outfiles[i], 'w') as outfile:\n",
    "                json.dump(dict_NSF[i], outfile, indent=4)\n",
    "    def csv_dump():\n",
    "        dict_NSF = [awards,\n",
    "                    abstracts,\n",
    "                    institutions,\n",
    "                    PI]\n",
    "\n",
    "        outfiles = [\"NSF_AWARDS.csv\",\n",
    "                    \"NSF_ABSTRACTS.csv\",\n",
    "                    \"NSF_INSTITUTIONS.csv\",\n",
    "                    \"NSF_PI.csv\"]\n",
    "\n",
    "        for _dict,file in zip(dict_NSF,outfiles):\n",
    "            with open(file, 'w') as output_file:\n",
    "                dict_writer = csv.DictWriter(output_file, _dict[0].keys())\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows(_dict)\n",
    "\n",
    "    def remove_duplicates(dict_list):\n",
    "        return [dict(tupleized) for tupleized in set(tuple(item.items()) for item in dict_list)]\n",
    "\n",
    "    # Create global lists.\n",
    "    awards = []\n",
    "    abstracts = []\n",
    "    institutions = []\n",
    "    PI = []\n",
    "    # scrape the files\n",
    "    start = time.time()\n",
    "    for path in paths:\n",
    "        scrape(path)\n",
    "    print(len(awards),\"records\")\n",
    "    print(\"Scraped in\", round(time.time()-start, 2), \"sec.\")\n",
    "    print(\"****************\")\n",
    "    # remove doops\n",
    "    start = time.time()\n",
    "    awards = remove_duplicates(awards)\n",
    "    PI = remove_duplicates(PI)\n",
    "    institutions = remove_duplicates(institutions)\n",
    "    print(\"Duplicates removed in\",round(time.time()-start,2), \"sec.\")\n",
    "    # dump'em\n",
    "    start = time.time()\n",
    "    csv_dump()\n",
    "    print(\"Dumped into CSV in\",round(time.time()-start,2), \"sec.\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def SQLite_dump(infile,Type):\n",
    "    print(\"*********************************************************************\")\n",
    "    # open SQlite connection\n",
    "    disk_engine = sql.create_engine('sqlite:///NSF.db')\n",
    "    start = dt.datetime.now()\n",
    "    tableName = infile.replace(\"NSF_\",\"\").replace(\".csv\",\"\").replace(\".json\",\"\")\n",
    "    if(Type=='csv'):\n",
    "        chunksize = 20000\n",
    "        j = 0\n",
    "        # Break each CSV into chunks to avoid inmemory stoarge of dataset.\n",
    "        for df in pd.read_csv(infile, chunksize=chunksize, iterator=True, encoding='utf-8'):\n",
    "            if(tableName=='AWARDS'):\n",
    "                df['AwardEffectiveDate']  = pd.to_datetime(df['AwardEffectiveDate'])\n",
    "                df['AwardExpirationDate'] = pd.to_datetime(df['AwardExpirationDate'])\n",
    "            j+=1\n",
    "            print(\"{} seconds: {} records dumped into SQLite table {}\".format((dt.datetime.now() - start).seconds,j*chunksize,tableName))\n",
    "            if(j==1):\n",
    "                df.to_sql(tableName,disk_engine,if_exists='replace')\n",
    "            else:\n",
    "                df.to_sql(tableName,disk_engine,if_exists='append')\n",
    "            \n",
    "    elif(Type=='json'):\n",
    "        df = pd.read_json(infile)\n",
    "        if(tableName=='AWARDS'):\n",
    "            df['AwardEffectiveDate']  = pd.to_datetime(df['AwardEffectiveDate'])\n",
    "            df['AwardExpirationDate'] = pd.to_datetime(df['AwardExpirationDate'])\n",
    "        print(\"{} seconds: {} records dumped into SQLite table {}\".format((dt.datetime.now() - start).seconds,len(df),tableName))\n",
    "        df.to_sql(tableName,disk_engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************************************\n",
      "4 seconds: 15000 records dumped into SQLite table AWARDS\n",
      "9 seconds: 30000 records dumped into SQLite table AWARDS\n",
      "14 seconds: 45000 records dumped into SQLite table AWARDS\n",
      "20 seconds: 60000 records dumped into SQLite table AWARDS\n",
      "27 seconds: 75000 records dumped into SQLite table AWARDS\n",
      "36 seconds: 90000 records dumped into SQLite table AWARDS\n",
      "49 seconds: 105000 records dumped into SQLite table AWARDS\n",
      "55 seconds: 120000 records dumped into SQLite table AWARDS\n",
      "61 seconds: 135000 records dumped into SQLite table AWARDS\n",
      "67 seconds: 150000 records dumped into SQLite table AWARDS\n",
      "73 seconds: 165000 records dumped into SQLite table AWARDS\n",
      "78 seconds: 180000 records dumped into SQLite table AWARDS\n",
      "81 seconds: 195000 records dumped into SQLite table AWARDS\n",
      "*********************************************************************\n",
      "0 seconds: 15000 records dumped into SQLite table ABSTRACTS\n",
      "21 seconds: 30000 records dumped into SQLite table ABSTRACTS\n",
      "22 seconds: 45000 records dumped into SQLite table ABSTRACTS\n",
      "23 seconds: 60000 records dumped into SQLite table ABSTRACTS\n",
      "25 seconds: 75000 records dumped into SQLite table ABSTRACTS\n",
      "30 seconds: 90000 records dumped into SQLite table ABSTRACTS\n",
      "34 seconds: 105000 records dumped into SQLite table ABSTRACTS\n",
      "38 seconds: 120000 records dumped into SQLite table ABSTRACTS\n",
      "44 seconds: 135000 records dumped into SQLite table ABSTRACTS\n",
      "48 seconds: 150000 records dumped into SQLite table ABSTRACTS\n",
      "52 seconds: 165000 records dumped into SQLite table ABSTRACTS\n",
      "58 seconds: 180000 records dumped into SQLite table ABSTRACTS\n",
      "66 seconds: 195000 records dumped into SQLite table ABSTRACTS\n",
      "*********************************************************************\n",
      "0 seconds: 15000 records dumped into SQLite table INSTITUTIONS\n",
      "*********************************************************************\n",
      "0 seconds: 15000 records dumped into SQLite table PI\n",
      "1 seconds: 30000 records dumped into SQLite table PI\n",
      "2 seconds: 45000 records dumped into SQLite table PI\n",
      "3 seconds: 60000 records dumped into SQLite table PI\n",
      "4 seconds: 75000 records dumped into SQLite table PI\n",
      "4 seconds: 90000 records dumped into SQLite table PI\n"
     ]
    }
   ],
   "source": [
    "outfilesCSV  = [\"NSF_AWARDS.csv\",\n",
    "                \"NSF_ABSTRACTS.csv\",\n",
    "                \"NSF_INSTITUTIONS.csv\",\n",
    "                \"NSF_PI.csv\"]\n",
    "for infile in outfilesCSV:\n",
    "    SQLite_dump(infile,Type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of NSF 2015 \n",
    "Cool things to look at\n",
    "<br> Common words in titles\n",
    "<br> email each PI with a status update.\n",
    "<br> Chloropleth about instituional funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search case 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>AwardTitle</th>\n",
       "      <th>AwardEffectiveDate</th>\n",
       "      <th>AwardExpirationDate</th>\n",
       "      <th>AwardAmount</th>\n",
       "      <th>InstitutionName</th>\n",
       "      <th>Division</th>\n",
       "      <th>EmailAddress</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>567</td>\n",
       "      <td>Workshop on Value-Sensitive Design: Cultivatin...</td>\n",
       "      <td>2000-05-15 00:00:00.000000</td>\n",
       "      <td>2001-04-30 00:00:00.000000</td>\n",
       "      <td>35000</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Information &amp; Intelligent Systems</td>\n",
       "      <td>borning@cs.washington.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>717</td>\n",
       "      <td>Collaborative Research: A Sea Test of the PROD...</td>\n",
       "      <td>2000-01-15 00:00:00.000000</td>\n",
       "      <td>2000-12-31 00:00:00.000000</td>\n",
       "      <td>8295</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Ocean Sciences</td>\n",
       "      <td>johnson@ocean.washington.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>735</td>\n",
       "      <td>NEP GLOBEC: Mesoscale Euphausiid and Hake Dist...</td>\n",
       "      <td>2000-04-01 00:00:00.000000</td>\n",
       "      <td>2006-09-30 00:00:00.000000</td>\n",
       "      <td>520740</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Ocean Sciences</td>\n",
       "      <td>bhickey@u.washington.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>904</td>\n",
       "      <td>SGER: Calibration of FeTi-Oxide Thermobaromete...</td>\n",
       "      <td>2000-05-01 00:00:00.000000</td>\n",
       "      <td>2002-04-30 00:00:00.000000</td>\n",
       "      <td>19885</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Earth Sciences</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>990</td>\n",
       "      <td>Shipboard Scientific Support Equipment 2000</td>\n",
       "      <td>2000-07-15 00:00:00.000000</td>\n",
       "      <td>2002-06-30 00:00:00.000000</td>\n",
       "      <td>82496</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Ocean Sciences</td>\n",
       "      <td>mcduff@ocean.washington.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AwardID                                         AwardTitle  \\\n",
       "index                                                               \n",
       "207        567  Workshop on Value-Sensitive Design: Cultivatin...   \n",
       "247        717  Collaborative Research: A Sea Test of the PROD...   \n",
       "257        735  NEP GLOBEC: Mesoscale Euphausiid and Hake Dist...   \n",
       "281        904  SGER: Calibration of FeTi-Oxide Thermobaromete...   \n",
       "314        990        Shipboard Scientific Support Equipment 2000   \n",
       "\n",
       "               AwardEffectiveDate         AwardExpirationDate  AwardAmount  \\\n",
       "index                                                                        \n",
       "207    2000-05-15 00:00:00.000000  2001-04-30 00:00:00.000000        35000   \n",
       "247    2000-01-15 00:00:00.000000  2000-12-31 00:00:00.000000         8295   \n",
       "257    2000-04-01 00:00:00.000000  2006-09-30 00:00:00.000000       520740   \n",
       "281    2000-05-01 00:00:00.000000  2002-04-30 00:00:00.000000        19885   \n",
       "314    2000-07-15 00:00:00.000000  2002-06-30 00:00:00.000000        82496   \n",
       "\n",
       "                InstitutionName                           Division  \\\n",
       "index                                                                \n",
       "207    University of Washington  Information & Intelligent Systems   \n",
       "247    University of Washington                     Ocean Sciences   \n",
       "257    University of Washington                     Ocean Sciences   \n",
       "281    University of Washington                     Earth Sciences   \n",
       "314    University of Washington                     Ocean Sciences   \n",
       "\n",
       "                       EmailAddress  \n",
       "index                                \n",
       "207       borning@cs.washington.edu  \n",
       "247    johnson@ocean.washington.edu  \n",
       "257        bhickey@u.washington.edu  \n",
       "281                            None  \n",
       "314     mcduff@ocean.washington.edu  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DYNAMIC QUERRYING\n",
    "def sql_Querry(fromTable,db='NSF',fromCol=-1,WHERE=-1,LIMIT=-1,colsToDisplay='*'):\n",
    "    disk_engine = sql.create_engine('sqlite:///'+db+'.db') #open connection to SQLite\n",
    "    if(fromCol==-1 and LIMIT==-1):                 # general search\n",
    "        print(\"search case 1\")\n",
    "        querry = 'SELECT {} FROM {}'.format(colsToDisplay.replace('[',\"\").replace(']',\"\"),fromTable)\n",
    "    elif(fromCol==-1 and LIMIT!=-1): # general search with limit\n",
    "        print(\"search case 2\")\n",
    "        querry = 'SELECT {} FROM {} LIMIT {}'.format(colsToDisplay.replace('[',\"\").replace(']',\"\"),fromTable,LIMIT)\n",
    "    elif(fromCol!=-1 and WHERE!=-1 and LIMIT==-1): # search a column for a whereClause\n",
    "        print(\"search case 3\")\n",
    "        querry = 'SELECT {} FROM {} WHERE {}=\"{}\" COLLATE NOCASE'.format(colsToDisplay.replace('[',\"\").replace(']',\"\"),fromTable,fromCol,WHERE)\n",
    "    elif(fromCol!=-1 and WHERE!=-1 and LIMIT!=-1): # search a column for a whereClause with a limit\n",
    "        print(\"search case 4\")\n",
    "        querry = 'SELECT {} FROM {} WHERE {}=\"{}\" COLLATE NOCASE LIMIT {}'.format(colsToDisplay.replace('[',\"\").replace(']',\"\"),fromTable,fromCol,WHERE,LIMIT)\n",
    "    return pd.read_sql_query(querry, disk_engine,index_col='index')\n",
    "sql_Querry(db='NSF',fromTable='AWARDS',fromCol='InstitutionName',WHERE='University of Washington',LIMIT=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search case 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardID</th>\n",
       "      <th>AwardTitle</th>\n",
       "      <th>AwardEffectiveDate</th>\n",
       "      <th>AwardExpirationDate</th>\n",
       "      <th>AwardAmount</th>\n",
       "      <th>InstitutionName</th>\n",
       "      <th>Division</th>\n",
       "      <th>EmailAddress</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Regulation of Sn-Glycerol-3-Phosphate Metaboli...</td>\n",
       "      <td>1986-07-01 00:00:00.000000</td>\n",
       "      <td>1986-07-01 00:00:00.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Virginia Polytechnic Institute and State Unive...</td>\n",
       "      <td>Molecular and Cellular Bioscience</td>\n",
       "      <td>tilarson@vt.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Design of Cutting Tools for High Speed Milling</td>\n",
       "      <td>2000-06-15 00:00:00.000000</td>\n",
       "      <td>2004-05-31 00:00:00.000000</td>\n",
       "      <td>280000</td>\n",
       "      <td>University of Florida</td>\n",
       "      <td>Civil, Mechanical, &amp; Manufact Inn</td>\n",
       "      <td>jziegert@uncc.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AwardID                                         AwardTitle  \\\n",
       "index                                                               \n",
       "0            0  Regulation of Sn-Glycerol-3-Phosphate Metaboli...   \n",
       "1            9     Design of Cutting Tools for High Speed Milling   \n",
       "\n",
       "               AwardEffectiveDate         AwardExpirationDate  AwardAmount  \\\n",
       "index                                                                        \n",
       "0      1986-07-01 00:00:00.000000  1986-07-01 00:00:00.000000            0   \n",
       "1      2000-06-15 00:00:00.000000  2004-05-31 00:00:00.000000       280000   \n",
       "\n",
       "                                         InstitutionName  \\\n",
       "index                                                      \n",
       "0      Virginia Polytechnic Institute and State Unive...   \n",
       "1                                  University of Florida   \n",
       "\n",
       "                                Division       EmailAddress  \n",
       "index                                                        \n",
       "0      Molecular and Cellular Bioscience    tilarson@vt.edu  \n",
       "1      Civil, Mechanical, & Manufact Inn  jziegert@uncc.edu  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_Querry(fromTable='AWARDS',LIMIT=10) # same as df.head(10), but doesn't read all files into memory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search case 1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 185345 entries, 0 to 5344\n",
      "Data columns (total 8 columns):\n",
      "AwardID                185345 non-null int64\n",
      "AwardTitle             185342 non-null object\n",
      "AwardEffectiveDate     185345 non-null object\n",
      "AwardExpirationDate    185345 non-null object\n",
      "AwardAmount            185345 non-null int64\n",
      "InstitutionName        185345 non-null object\n",
      "Division               185345 non-null object\n",
      "EmailAddress           179320 non-null object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 12.7+ MB\n"
     ]
    }
   ],
   "source": [
    "sql_Querry(fromTable='AWARDS').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "award_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RDBA this works great... However, for publishing results nobody wants columns displayed_like_this! So here's a helper function to convert underscored columns into respectible, well-mannered headers. Appropriated from Stackoverflow user <a target=\"_blank\" href='http://stackoverflow.com/a/6425628/5094480'>Siegfried Gevatter</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beautify(underscored_word):\n",
    "    # check for acronym\n",
    "    if(len(underscored_word)>2):\n",
    "        return ' '.join(x.capitalize() or '_' for x in underscored_word.split('_'))\n",
    "    else:\n",
    "        return underscored_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beautify1(camelCaseWord):\n",
    "    # check for acronym\n",
    "    return(''.join(map(lambda x: x if x.islower() else \" \"+x, camelCaseWord)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, flexible reporting function to find the top n funded column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def top_funds(df,col,n=5):\n",
    "    return df.groupby(col).apply(lambda x:x['AwardAmount'].sum()).sort_values(ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "award_df.groupby('Division').apply(lambda x:x['AwardAmount'].sum()).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_awarded(df,col,n):\n",
    "    return df['Division'].value_counts().head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"University of Texas\".split()\n",
    "org_df[org_df['InstitutionName'].str.contains(\"|\".join(\"University of Texas\".split()),case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "award_df[award_df['InstitutionName'].str.contains('New York',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_funds(award_df[award_df['InstitutionName'].str.contains('Yale',case=False)],'Division',10)\n",
    "# if unique names are more than 1, show results for one with greatest hits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_awarded(award_df,'Division',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def report(InstitutionName,Division):\n",
    "    return award_df[(award_df.InstitutionName==InstitutionName) & \n",
    "                    (award_df.Division==Division)].merge(abstract_df,on='AwardID').set_index('AwardID')[\n",
    "                    [\"AwardTitle\",\"EmailAddress\",\"Abstract\"]]\n",
    "\n",
    "report(\"University of Washington\",\"Ocean Sciences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topics_finder(topic):\n",
    "    df = pd.merge(left=award_df,right=abstract_df,on='AwardID')\n",
    "    #df = (df.loc[(df['AwardTitle'].notnull()) & (df['Abstract'].notnull()) &\n",
    "    #             (df['AwardTitle'].str.contains(topic,case=False)) | # search title\n",
    "    #              df['Abstract'].str.contains(topic,case=False)])    # search abstract SLOW!\n",
    "    \n",
    "    df = (df.loc[(df['AwardTitle'].notnull()) &\n",
    "                 (df['AwardTitle'].str.contains(topic,case=False))])\n",
    "    #df = pd.merge(left=df,right=pi_df, on='EmailAddress')            # get PI info\n",
    "    return(df[['AwardID','AwardTitle','AwardEffectiveDate','EmailAddress']])\n",
    "topics_finder(\"omics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PI_Report(PI):\n",
    "    # search email of full name\n",
    "    try:\n",
    "        return award_df.loc[award_df.EmailAddress.isin(pi_df.EmailAddress[pi_df.FullName==PI.lower()].values)]\n",
    "        #return df.AwardTitle.values\n",
    "    except:\n",
    "        return(PI,\"Not found in Our database\")\n",
    "    # search awards for email\n",
    "    # display states\n",
    "PI_Report('nicole lovenduski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "award_df[award_df.EmailAddress=='jig@ldeo.columbia.edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO fix PI shared names with email addresses!\n",
    "def top_PI():\n",
    "    award_df['PI'] = award_df.first_name+\" \"+award_df.last_name\n",
    "    pi_df['PI'] = pi_df.first_name+\" \"+pi_df.last_name\n",
    "    top_fund_col = []\n",
    "    for PI in pi_df.PI:\n",
    "        top_fund_col.append(\n",
    "            {'PI':PI, 'funding_division': np.asarray(award_df[award_df.PI==PI].division)[0],\"Total_Award_Money\" : award_df[award_df['PI']==PI].award_amount.sum()})\n",
    "    fund_df = pd.DataFrame(sorted(top_fund_col, key=lambda k: k['Total_Award_Money'], reverse=True))\n",
    "    fund_df = pd.merge(left=fund_df,right=pi_df,on='PI',how='inner')\n",
    "    fund_df.index = fund_df.index+1\n",
    "    fund_df.rename(columns=lambda x: beautify(x), inplace=True)\n",
    "    return fund_df[[\"Total Award Money\",\"PI\",\"Email Address\",\"Funding Division\"]][:25].drop_duplicates()\n",
    "top_PI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "Short term:\n",
    "Make time series of funding\n",
    "University profile(maybe \n",
    "Chunk Pandas for scale?\n",
    "Use SQLite rather than SQL? (maybe)...\n",
    "Bokeh interactive plots (or D3.js)\n",
    "\n",
    "Long term:\n",
    "PI's who switch schools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
