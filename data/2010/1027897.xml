<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CDI-Type II: Collaborative Research: Perception of Scene Layout by Machines and Visually Impaired Users</AwardTitle>
    <AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2015</AwardExpirationDate>
    <AwardAmount>249507</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>01060000</Code>
      <Directorate>
        <LongName>Office Of The Director</LongName>
      </Directorate>
      <Division>
        <LongName>Office of Integrative Activities</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Stephen Meacham</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The project investigates computational methods for object detection, spatial scene construction, and natural language spatial descriptions derived from real-time visual images to describe prototypical indoor spaces (e.g., rooms, offices, etc.). The primary application of this research is to provide blind or visually impaired users with spatial information about their surroundings that may otherwise be difficult to obtain from non-visual sensing. Such knowledge will assist in development of accurate cognitive models of the environment and will support better informed execution of spatial behaviors in everyday tasks. &lt;br/&gt;&lt;br/&gt;A second motivation for the work is to contribute to the improvement of spatial capacities for computers and robots. Computers and robots are similarly "blind" to images unless they have been provided some means to "see" and understand them. Currently, no robotic system is able to reliably perform high-level processing of spatial information on the basis of image sequences, e.g., to find an empty chair in a room, which not only means finding an empty chair in an image, but also localizing the chair in the room, and performing an action of reaching the chair. The guiding tenet of this research is that a better understanding of spatial knowledge acquisition from visual images and concepts of spatial awareness by humans can also be applied to reducing the ambiguity and uncertainty of information processing by autonomous systems. &lt;br/&gt;&lt;br/&gt;A central contribution of this work is to make the spatial information content of visual images available to the visually impaired, a rapidly growing demographic of our aging society. In an example scenario a blind person and her guide dog are walking to her doctor's office, an office which she has not previously visited. At the office she needs information for performing some essential tasks such as finding the check-in counter, available seating, or the bathroom. No existing accessible navigation systems are able to describe the spatial parameters of an environment and help detect and localize objects in that space. Our work will provide the underlying research and elements to realize such a system.</AbstractNarration>
    <MinAmdLetterDate>09/20/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>09/20/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1027897</AwardID>
    <Investigator>
      <FirstName>Longin Jan</FirstName>
      <LastName>Latecki</LastName>
      <EmailAddress>latecki@temple.edu</EmailAddress>
      <StartDate>09/20/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Temple University</Name>
      <CityName>PHILADELPHIA</CityName>
      <ZipCode>191405104</ZipCode>
      <PhoneNumber>2157077379</PhoneNumber>
      <StreetAddress>3340 N. Broad Street</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7751</Code>
      <Text>CDI TYPE II</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7721</Code>
      <Text>FROM DATA TO KNOWLEDGE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5936</Code>
      <Text>GERMANY (F.R.G.)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5979</Code>
      <Text>Europe and Eurasia</Text>
    </ProgramReference>
  </Award>
</rootTag>
