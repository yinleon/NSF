<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Plan Execution for Continuous Dynamical Systems Within Risk Bounds</AwardTitle>
    <AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>298807</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Todd Leen</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>In many applications, from undersea to space, an autonomous agent is given a set of operational goals to achieve optimally, while taking into account the uncertainties that arise from uncontrollable events. For example, in a monitoring mission for an autonomous under-water vehicle (AUV), the goal might be to maximize scientific return, while avoiding hazards. Due to uncertainty, it is unrealistic for many real-world mission to guarantee 100% success. One approach explored extensively within the decision-theoretic planning community is to maximize an objective that trades risk for utility. However, this does not provide any hard guarantees. An alternative approach, commonly employed in engineering practice is to specify risk as a hard constraint, in terms of an upper bound on mission failure (called a chance constraint). For example, NASA Mars missions are designed to meet or exceed a requirement on the probability of successful landing; human-rated vehicles are designed to similar requirements. Given a chance-constrained mission, an agent performing the mission may strive to maximize expected reward, while ensuring that the chance constraint and other operating constraints are met. &lt;br/&gt;&lt;br/&gt;This research is developing a model-based executive that achieves goal-level plans within specified risk bounds, while attempting to maximize expected reward. Key attributes of this executive include: 1) user specification of time-evolved goal behaviors; 2) plan execution by generating a sequence of discrete and continuous actions for controlling the plant; and 3) optimal, stochastic planning of control actions within risk bounds and dynamic constraints. The research under this grant is laying the groundwork for longer-term objectives of facilitating continuous adaptation of the initial plan and allocation of risk as uncertainties are resolved during plan execution. Among other applications, we plan tests with AUVs engaged in scientific missions, measuring performance within obstacle-related, time and energy bounds.</AbstractNarration>
    <MinAmdLetterDate>09/13/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>09/13/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1017992</AwardID>
    <Investigator>
      <FirstName>Brian</FirstName>
      <LastName>Williams</LastName>
      <EmailAddress>williams@mit.edu</EmailAddress>
      <StartDate>09/13/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Massachusetts Institute of Technology</Name>
      <CityName>Cambridge</CityName>
      <ZipCode>021394301</ZipCode>
      <PhoneNumber>6172531000</PhoneNumber>
      <StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Massachusetts</StateName>
      <StateCode>MA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
  </Award>
</rootTag>
