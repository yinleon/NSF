<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SBIR Phase II: Multimodal Semantic Video Retrieval and Summarization</AwardTitle>
    <AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>03/31/2013</AwardExpirationDate>
    <AwardAmount>705999</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate for Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Industrial Innovation and Partnerships</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Glenn H. Larsen</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This Small Business Innovation Research Phase II project will develop contextual video segmentation and automatic tagging technology and software. In long video streams that contain one or more topics, the software automatically discovers the beginnings and ends of Contextually-Coherent Video Segments in each video. Moreover, Video Semantics' technology automatically assigns textual tags to each segment such that these tags describe the topic discussed in that segment. The tags assigned make all parts of the video easily searchable. Large video producers currently depend on manually segmenting their content into small segments and assigning textual tags to these segments in order to make them searchable. A short advertisement is then inserted before each segment. This manual segmentation and tagging process represents a significant pain point for content producers because it is labor intensive and not cost effective. Meanwhile, government agencies, which continuously monitor video content depend on speech recognition to spot specified keywords. This approach inflicts two pain points: (i) analysts have to deal with large number of false detections because the context in which the keyword occurs might be irrelevant, and (ii) if the keyword occurs in an important context, analysts still need to scroll back and forth into the video to find the beginning of the relevant segment. &lt;br/&gt;&lt;br/&gt;Video Semantics' technology and products have the potential to efficiently address significant market needs. In addition to the commercial applications, the proposed technology will enable media monitoring agencies to perform their tasks more efficiently saving valuable analyst time and resources. Moreover, because Video Semantics? technology is language-independent, media monitoring agencies will be able to monitor more content in foreign languages without the need to develop language-specific technologies. The company will employ an indirect sales strategy via partnerships with software companies that develop media monitoring solutions and metadata generation tools. The company has identified its first customer and is working with them to integrate the contextual segmentation and tagging technology with their current media monitoring solutions.</AbstractNarration>
    <MinAmdLetterDate>03/23/2011</MinAmdLetterDate>
    <MaxAmdLetterDate>12/18/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1058428</AwardID>
    <Investigator>
      <FirstName>Wael</FirstName>
      <LastName>Abd-Almageed</LastName>
      <EmailAddress>wamageed@videosemantics.com</EmailAddress>
      <StartDate>03/23/2011</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Video Semantics LLC</Name>
      <CityName>Ellicott City</CityName>
      <ZipCode>210430000</ZipCode>
      <PhoneNumber>3013186427</PhoneNumber>
      <StreetAddress>3565 A2 Ellicott Mills Dr</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Maryland</StateName>
      <StateCode>MD</StateCode>
    </Institution>
  </Award>
</rootTag>
