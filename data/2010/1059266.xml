<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CI-P Seeing speech: A community resource for analysis of multi-modal language data</AwardTitle>
    <AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>02/28/2013</AwardExpirationDate>
    <AwardAmount>100000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05050000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computer and Network Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Producing human speech requires the exquisite timing of multiple interacting modalities: movements of the lips, tongue, velum, and vocal folds are all precisely coordinated to give each sound its unique properties. Understanding how these different modalities interact is important to basic linguistic and cognitive science as well as to applied research areas, such as automatic speech recognition, speech therapy, and second language learning. This workshop will bring together experts in speech database construction, speech recognition, and articulatory research to explore the feasibility and desirability of developing software and a database to study the interaction of the speech articulators and how their coordination relates to the sounds produced. The workshop brings these three communities together to address (i) requirements of software for extracting and analyzing articulatory data in conjunction with the acoustic signal, and (ii) properties of the database to be constructed, namely: which utterances should be recorded; how to synchronize capture of the multiple modalities; markup, annotation, and storage of the data.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The goal of the project is to develop both software for multi-modal speech analysis and a database of synchronized multi-modal speech recordings. A critical first step is to assess which features of the database and software are needed to maximize the long-term value to the scientific community. This workshop brings together researchers from diverse backgrounds in human language and computer sciences to examine these issues, so that the database and software may facilitate studies of oral tract articulation across many disciplines, e.g. to understand the diversity of human language sounds, language acquisition and endangered languages, explore speech deficits, teach foreign language pronunciation or oral language to the profoundly deaf, improve speech recognition and synthesis software, understand how musicians shape sounds while playing wind instruments, etc.</AbstractNarration>
    <MinAmdLetterDate>02/18/2011</MinAmdLetterDate>
    <MaxAmdLetterDate>02/18/2011</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1059266</AwardID>
    <Investigator>
      <FirstName>Diana</FirstName>
      <LastName>Archangeli</LastName>
      <EmailAddress>dba@email.arizona.edu</EmailAddress>
      <StartDate>02/18/2011</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Ian</FirstName>
      <LastName>Fasel</LastName>
      <EmailAddress>ianfasel@sista.arizona.edu</EmailAddress>
      <StartDate>02/18/2011</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Arizona</Name>
      <CityName>TUCSON</CityName>
      <ZipCode>857210001</ZipCode>
      <PhoneNumber>5206266000</PhoneNumber>
      <StreetAddress>888 N Euclid Ave</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Arizona</StateName>
      <StateCode>AZ</StateCode>
    </Institution>
  </Award>
</rootTag>
