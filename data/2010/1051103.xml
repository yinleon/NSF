<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>EAGER: Spontaneous 4D-Facial Expression Corpus for Automated Facial Image Analysis</AwardTitle>
    <AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2012</AwardExpirationDate>
    <AwardAmount>65784</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Currently, few publically available, annotated databases exist. Those that do are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate training data. Because posed and un-posed (aka ?spontaneous?) facial expressions differ along several dimensions including complexity, well annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.&lt;br/&gt;&lt;br/&gt;This project develops a 3D video corpus of spontaneous facial and vocal expression in a diverse group of young adults. Well-validated emotion inductions elicit expressions of emotion and paralinguistic communication. Sequence-level ground truth is obtained via participant self-report. Frame-level ground-truth is obtained via facial action unit coding using the Facial Action Coding System. The project promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.&lt;br/&gt;&lt;br/&gt;The project promotes research on next-generation affective computing with applications in security, law-enforcement, biomedicine, behavior science, entertainment and education. The multimodal 3D video database and its metadata are for the research community for new algorithm development, assessment, comparison, and evaluation.</AbstractNarration>
    <MinAmdLetterDate>08/24/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>08/24/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1051103</AwardID>
    <Investigator>
      <FirstName>Lijun</FirstName>
      <LastName>Yin</LastName>
      <EmailAddress>lijun@cs.binghamton.edu</EmailAddress>
      <StartDate>08/24/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>SUNY at Binghamton</Name>
      <CityName>BINGHAMTON</CityName>
      <ZipCode>139026000</ZipCode>
      <PhoneNumber>6077776136</PhoneNumber>
      <StreetAddress>4400 VESTAL PKWY E</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
  </Award>
</rootTag>
