<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>HCC: Small: Depth Perception in Near- and Medium-Field Augmented Reality</AwardTitle>
    <AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2015</AwardExpirationDate>
    <AwardAmount>499998</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Ephraim P. Glinert</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Project Abstract&lt;br/&gt;HCC: Small: Depth Perception in Near- and Medium-Field Augmented Reality&lt;br/&gt;&lt;br/&gt;Augmented reality (AR) is a technology where users see computer-generated, virtual objects superimposed on their view of the world. A large number of compelling AR applications have been proposed and developed in areas such as manufacturing, logistics, maintenance, situation awareness, heads-up maps, training, tourism, archeology, theater, choreography, architecture, and image-guided surgery. A common challenge for all of these AR applications is placing virtual objects at a specific depth in the real world. Because of inherent engineering limitations, AR displays do not show virtual objects with the same depth cues as real objects. Furthermore, some AR applications involve x-ray vision, which is the display of virtual objects that exist behind opaque surfaces. For these applications, the depth cues necessarily conflict, but still yield useful information. This leads to the scientific problems that this project is studying: (1) how depth perception operates with the conflicting depth cues of AR, and (2) new methods that can more effectively convey AR depth need to be developed and validated. &lt;br/&gt;&lt;br/&gt;The investigators are addressing these problems through two major project tracks. The first track is empirically studying how the depth of computer-generated images is perceived for near-field distances within arm?s reach and medium-field distances of 1.5 to 30 meters. The second track is implementing and empirically testing several new eye tracker-based AR depth presentation methods. These include vergence depth rendering, where the vergence angle of the eyes controls which AR objects are visible, and simulated depth-of-field, where objects at different depths than the vergence angle are blurred. This simulates depth-of-field blurring in an AR display with a fixed focal depth. The project?s experiments adopt empirical methods from the long history of depth perception research and use control conditions that allow validation through comparison with this literature.</AbstractNarration>
    <MinAmdLetterDate>07/31/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>07/31/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1018413</AwardID>
    <Investigator>
      <FirstName>J. Edward</FirstName>
      <LastName>Swan II</LastName>
      <EmailAddress>swan@cse.msstate.edu</EmailAddress>
      <StartDate>07/31/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Mississippi State University</Name>
      <CityName>MISSISSIPPI STATE</CityName>
      <ZipCode>397629662</ZipCode>
      <PhoneNumber>6623257404</PhoneNumber>
      <StreetAddress>PO Box 6156</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Mississippi</StateName>
      <StateCode>MS</StateCode>
    </Institution>
  </Award>
</rootTag>
